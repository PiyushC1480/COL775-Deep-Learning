{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"execution":{"iopub.execute_input":"2024-04-03T19:43:50.249218Z","iopub.status.busy":"2024-04-03T19:43:50.248464Z","iopub.status.idle":"2024-04-03T19:44:00.903998Z","shell.execute_reply":"2024-04-03T19:44:00.903238Z","shell.execute_reply.started":"2024-04-03T19:43:50.249163Z"},"trusted":true},"outputs":[],"source":["import torch\n","import torch.nn as nn\n","from torchtext.vocab import GloVe\n","import torch.optim as optim\n","import pickle\n","import os\n","import random\n","import nltk\n","import json\n","import re\n","import numpy as np\n","import pandas as pd\n","import subprocess\n","\n","from nltk import word_tokenize\n","from torch.utils.data import Dataset, DataLoader\n","from collections import defaultdict\n","from time import time\n","from transformers import BertTokenizer\n","from transformers import BertModel\n","\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","\n","class ARGS():\n","    def __init__(self):\n","        self.model_type = \"lstm_lstm\"\n","        self.data_dir = \"/kaggle/input/math-data\"\n","        self.batch_size = 32\n","        self.num_workers = 4\n","        self.epochs = 80\n","        self.en_hidden = 512\n","        self.de_hidden = 512\n","        self.en_num_layers = 1\n","        self.de_num_layers = 1\n","        self.embed_dim = 300\n","        self.processed_data = \"/kaggle/input/math-data\"\n","        self.checkpoint_dir = \"/kaggle/working/checkpoints\"\n","        self.result_dir = \"/kaggle/working/results\""]},{"cell_type":"code","execution_count":2,"metadata":{"execution":{"iopub.execute_input":"2024-04-03T19:44:00.906051Z","iopub.status.busy":"2024-04-03T19:44:00.905576Z","iopub.status.idle":"2024-04-03T19:44:00.910592Z","shell.execute_reply":"2024-04-03T19:44:00.909644Z","shell.execute_reply.started":"2024-04-03T19:44:00.906026Z"},"trusted":true},"outputs":[],"source":["os.mkdir(\"/kaggle/working/results\")\n","os.mkdir(\"/kaggle/working/checkpoints\")"]},{"cell_type":"code","execution_count":3,"metadata":{"execution":{"iopub.execute_input":"2024-04-03T19:44:00.912024Z","iopub.status.busy":"2024-04-03T19:44:00.911761Z","iopub.status.idle":"2024-04-03T19:44:00.928350Z","shell.execute_reply":"2024-04-03T19:44:00.927381Z","shell.execute_reply.started":"2024-04-03T19:44:00.912003Z"},"trusted":true},"outputs":[],"source":["# import shutil\n","# shutil.rmtree(\"/kaggle/working/results\")\n","# shutil.rmtree(\"/kaggle/working/checkpoints\")"]},{"cell_type":"markdown","metadata":{},"source":["UTILS"]},{"cell_type":"code","execution_count":4,"metadata":{"execution":{"iopub.execute_input":"2024-04-03T19:44:00.929928Z","iopub.status.busy":"2024-04-03T19:44:00.929660Z","iopub.status.idle":"2024-04-03T19:44:00.948722Z","shell.execute_reply":"2024-04-03T19:44:00.947942Z","shell.execute_reply.started":"2024-04-03T19:44:00.929906Z"},"trusted":true},"outputs":[],"source":["def tokenize_problem(problem):\n","    return word_tokenize(problem)\n","\n","def tokenize_formula(formula):\n","    all_formulas = formula.split(\"|\")\n","    formula_split = []\n","    for s in all_formulas:\n","        s_brk = re.match(r\"([a-zA-Z]+)\\(([^,]+),([^)]+)\\)\", s)\n","        if s_brk:\n","            formula_split.extend([s_brk.group(1),\"(\", s_brk.group(2),\",\", s_brk.group(3), \")\",\"|\"])\n","    return formula_split\n","\n","\n","def build_vocab(pth):\n","    # make file encoder.vocab dn decoder.vocab \n","    # data is the directory which contain train.json\n","    \n","    encoder_vocab = set()\n","    decoder_vocab = set()\n","    encoder_word2idx = {}\n","    encoder_idx2word = {}\n","    decoder_word2idx = {}\n","    decoder_idx2word = {}\n","    pth2 = os.path.join(pth, \"train.json\")\n","    data = json.load(open(pth2))\n","    for d in data:\n","        problem = d[\"Problem\"]\n","        linear_formula = d[\"linear_formula\"]\n","        problem = tokenize_problem(problem)\n","        linear_formula = tokenize_formula(linear_formula)\n","        for p in problem:\n","            encoder_vocab.add(p)\n","        for l in linear_formula:\n","            decoder_vocab.add(p)\n","\n","    encoder_vocab = list(encoder_vocab)\n","    decoder_vocab = list(decoder_vocab)\n","    encoder_vocab.sort()\n","    decoder_vocab.sort()\n","\n","    for i, word in enumerate(encoder_vocab):\n","        encoder_word2idx[word] = i\n","        encoder_idx2word[i] = word\n","    for i, word in enumerate(decoder_vocab):\n","        decoder_word2idx[word] = i\n","        decoder_idx2word[i] = word\n","\n","    with open(os.path.join(pth, \"encoder.vocab\"), \"x\") as file:\n","        for word in encoder_vocab:\n","            file.write(word + \"\\n\")\n","    with open(os.path.join(pth, \"decoder.vocab\"), \"x\") as file:\n","        for word in decoder_vocab:\n","            file.write(word + \"\\n\")\n","\n","    # make pickle file for word2idx and idx2word\n","    with open(os.path.join(pth, \"encoder_word2idx.pickle\"), \"xb\") as file:\n","        pickle.dump(encoder_word2idx, file)\n","    with open(os.path.join(pth, \"encoder_idx2word.pickle\"), \"xb\") as file:\n","        pickle.dump(encoder_idx2word, file)\n","    with open(os.path.join(pth, \"decoder_word2idx.pickle\"), \"xb\") as file:\n","        pickle.dump(decoder_word2idx, file)\n","    with open(os.path.join(pth, \"decoder_idx2word.pickle\"), \"xb\") as file:\n","        pickle.dump(decoder_idx2word, file)\n","\n","    print(\"Encoder Vocab Size = {}, Decoder Vocab Size = {}\".format(len(encoder_vocab), len(decoder_vocab)) )\n","    print(\"Encoder word2idx Size = {}, Decoder word2idx Size = {}\".format(len(encoder_word2idx), len(decoder_word2idx)))\n","\n","def load_checkpoint(args, chkpt = \"best\"):\n","\n","    if chkpt == \"best\":\n","        model_name = os.path.join(args.checkpoint_dir, \"best_loss_checkpoint_{}.pth\".format(args.model_type))\n","        status_file = os.path.join(args.checkpoint_dir, \"best_loss_chkpt_status_{}.json\".format(args.model_type))\n","    else:\n","        model_name = os.path.join(args.checkpoint_dir, \"latest_checkpoint_{}.pth\".format(args.model_type))\n","        status_file = os.path.join(args.checkpoint_dir, \"latest_chkpt_status_{}.json\".format(args.model_type))\n","\n","    assert os.path.isfile(model_name), f\"Model path/name invalid: {model_name}\"\n","    \n","    net = torch.load(model_name)\n","    with open(status_file, \"r\") as file:\n","        model_dict = json.load(file)\n","    print(f\"\\n|--------- Model Load Success. Trained Epoch: {str(model_dict['epoch'])}\")\n","\n","    return net\n"]},{"cell_type":"markdown","metadata":{},"source":["ENCODER BLOCK"]},{"cell_type":"code","execution_count":5,"metadata":{"execution":{"iopub.execute_input":"2024-04-03T19:44:00.951564Z","iopub.status.busy":"2024-04-03T19:44:00.951297Z","iopub.status.idle":"2024-04-03T19:44:00.972091Z","shell.execute_reply":"2024-04-03T19:44:00.971214Z","shell.execute_reply.started":"2024-04-03T19:44:00.951543Z"},"trusted":true},"outputs":[],"source":["SPL_TOKEN = [\"<pad>\", \"<unk>\", \"<sos>\", \"<eos>\"]\n","\n","class GloveEmbeddings():\n","    def __init__(self, embed_dim,  word_to_idx):\n","        self.embed_dim = embed_dim\n","        self.word_to_idx = word_to_idx\n","        self.spl_tokens = SPL_TOKEN\n","        self.vocab_size = len(word_to_idx)\n","\n","    def get_embedding_matrix(self):\n","        # Load pre-trained GloVe embeddings\n","        glove = GloVe(name='6B', dim=self.embed_dim)\n","        embed_matrix = torch.zeros((self.vocab_size, self.embed_dim))\n","\n","        embed_matrix[0] = torch.zeros(self.embed_dim)    # Padding token\n","        for i in range(1,len(SPL_TOKEN)):            \n","            embed_matrix[i] = torch.randn(self.embed_dim)    # Start-of-sentence token\n","            \n","        for k, v in self.word_to_idx.items():\n","            if k in SPL_TOKEN:\n","                continue\n","            else:            \n","                if k in glove.stoi:\n","                    embed_matrix[v] = torch.tensor(glove.vectors[glove.stoi[k]])\n","                else:\n","                    embed_matrix[v] = embed_matrix[1]\n","        return embed_matrix\n","\n","\n","class LSTMEncoder(nn.Module):\n","    # Bidirectional LSTM Encoder\n","    def __init__(self, input_size , embed_dim, hidden_units =1024, num_layers = 1, embed_matrix = None, p = 0.3):\n","        super(LSTMEncoder, self).__init__()\n","        # parameters\n","        self.input_size = input_size\n","        self.embed_dim = embed_dim\n","        self.hidden_units = hidden_units\n","        self.num_layers = num_layers\n","        self.dropout = nn.Dropout(p)\n","        self.embed_matrix = embed_matrix\n","        self.embedding = nn.Embedding(input_size, embed_dim)\n","        if embed_matrix is not None:\n","            self.embedding = nn.Embedding.from_pretrained(embed_matrix)\n","        else:\n","            self_embedding  = nn.Embedding(input_size, embed_dim, padding_idx = 0)\n","\n","        self.LSTM = nn.LSTM(input_size=embed_dim, hidden_size=hidden_units, num_layers = num_layers, batch_first = True, bidirectional = True)\n","        self.hidden = nn.Linear(2*hidden_units, hidden_units)   \n","        self.cell = nn.Linear(2*hidden_units, hidden_units)\n","        \n","\n","    def forward(self,x):\n","        # apply dropout to the embeddings \n","        x = self.dropout(self.embedding(x))\n","        # apply LSTM\n","        output, (hidden,cell) = self.LSTM(x)\n","        hidden = self.hidden(torch.cat((hidden[0:1], hidden[1:2]), dim = 2)) # concatenate the forward and backward LSTM hidden states\n","        cell = self.cell(torch.cat((cell[0:1], cell[1:2]), dim = 2)) # concatenate the forward and backward LSTM cell states\n","        return output, (hidden, cell)\n","    \n","\n","#BERT encoder\n","class BertEncoder(nn.Module):\n","    def __init__(self, model_type, bert_tune_layers, hidden_units=1024):\n","        super(BertEncoder, self).__init__()\n","        self.model_type = model_type\n","        self.hidden_units = hidden_units\n","        self.bert_tune_layers = bert_tune_layers\n","        bert_model = BertModel.from_pretrained('bert-base-cased')\n","\n","        if self.model_type == \"bert_lstm_attn_frozen\":\n","            print(\"BERT Encoder with frozen embeddings.\")\n","            for name, param in bert_model.named_parameters():\n","                param.requires_grad = False\n","    \n","        elif self.model_type == \"bert_lstm_attn_tuned\":\n","            self.bert_tune_layers = bert_tune_layers\n","            print(\"BERT Encoder with tuned embeddings.\")\n","            if self.bert_tune_layers != -1:\n","                for param in bert_model.parameters():\n","                    param.requires_grad = False\n","                for param in bert_model.encoder.layer[-self.bert_tune_layers:].parameters():\n","                    param.requires_grad = True\n","            \n","        print(\"Total Bert Params = {}, Total Trainable Params = {}\".format(sum(p.numel() for p in bert_model.parameters()), sum(p.numel() for p in bert_model.parameters() if p.requires_grad)))\n","        self.encoder = bert_model\n","\n","    def forward(self, x, attn_mask):\n","        outputs = self.encoder(input_ids = x, attention_mask = attn_mask)\n","        encodings = outputs.last_hidden_state\n","        return encodings"]},{"cell_type":"markdown","metadata":{},"source":["DECODER BLOCK"]},{"cell_type":"code","execution_count":6,"metadata":{"execution":{"iopub.execute_input":"2024-04-03T19:44:00.973771Z","iopub.status.busy":"2024-04-03T19:44:00.973444Z","iopub.status.idle":"2024-04-03T19:44:01.001093Z","shell.execute_reply":"2024-04-03T19:44:01.000155Z","shell.execute_reply.started":"2024-04-03T19:44:00.973743Z"},"trusted":true},"outputs":[],"source":["class LSTMDecoder(nn.Module):\n","    def __init__(self, input_size, embed_dim, hidden_units = 1024, num_layers = 1, p = 0.3):\n","        super(LSTMDecoder, self).__init__()\n","        # parameters\n","        self.input_size = input_size\n","        self.embed_dim = embed_dim\n","        self.hidden_units = hidden_units\n","        self.num_layers = num_layers\n","        self.dropout = nn.Dropout(p)\n","        self.embedding = nn.Embedding(input_size, embed_dim)\n","        self.LSTM = nn.LSTM(input_size = embed_dim, hidden_size = hidden_units, num_layers = num_layers, batch_first = True)\n","        self.fc = nn.Linear(in_features = hidden_units, out_features = input_size)\n","    def forward(self, x, hidden_cell):\n","        # apply dropout to the embeddings\n","        x = self.dropout(self.embedding(x))\n","        x = x.unsqueeze(1) # unsqueeze the embeddings to add a dimension\n","        # apply LSTM\n","        out, (h_t, c_t) = self.LSTM(x, hidden_cell)\n","        out = self.fc(out)\n","        return out, (h_t, c_t)\n","    \n","class AttentionNetwork(nn.Module):\n","    def __init__(self, hidden_units):\n","        super(AttentionNetwork, self).__init__()\n","        self.hidden_units = hidden_units\n","        self.relu = nn.ReLU()\n","        self.softmax = nn.Softmax(dim=1)\n","        self.attn = nn.Linear(hidden_units * 3, hidden_units)\n","        self.v = nn.Linear(hidden_units, 1, bias=False)\n","\n","    def forward(self, ht, encoder_out):\n","        ht = ht.repeat(encoder_out.shape[1], 1 , 1)\n","        ht = ht.transpose(0,1)\n","        energy = torch.cat([ht, encoder_out], dim=2)\n","        energy = self.attn(energy)\n","        energy = self.relu(energy)        \n","        attention = self.v(energy)\n","        attention = attention.squeeze(2)\n","        attention_weights = self.softmax(attention)\n","        attention_weights = attention_weights.unsqueeze(1)\n","        context = torch.bmm(attention_weights, encoder_out)\n","        return context, attention_weights\n","    \n","class LSTMAttnDecoder(nn.Module):\n","    def __init__(self, input_size, embed_dim, hidden_units=1024, num_layers=1, p = 0.3, bidirectional=False):\n","        super(LSTMAttnDecoder, self).__init__()\n","        self.input_size = input_size\n","        self.embed_dim = embed_dim\n","        self.hidden_units = hidden_units\n","        self.num_layers = num_layers\n","        self.bidirectional = bidirectional\n","        self.dropout = nn.Dropout(p)\n","        self.embedding = nn.Embedding(input_size, self.embed_dim, padding_idx=0)\n","        self.attention = AttentionNetwork(hidden_units)\n","        self.LSTM = nn.LSTM(2*hidden_units + embed_dim, hidden_units, num_layers = num_layers, bidirectional = bidirectional, dropout=p, batch_first=True)\n","        self.fc = nn.Linear(hidden_units, input_size)\n","\n","    def forward(self, x, h0_c0, encoder_out):\n","        x = self.dropout(self.embedding(x)) #\n","        x = x.unsqueeze(1)\n","        context, _ = self.attention(h0_c0[0], encoder_out)\n","        x = torch.cat([x, context], dim=2)\n","        decoder_out, (ht, ct) = self.LSTM(x, h0_c0)\n","        out = self.fc(decoder_out)\n","        return out, (ht, ct)\n","    \n","\n","class AttentionNetworkBert(nn.Module):\n","    def __init__(self, hidden_units):\n","        super(AttentionNetworkBert, self).__init__()\n","        self.hidden_units = hidden_units\n","        self.relu = nn.ReLU()\n","        self.softmax = nn.Softmax(dim=1)\n","        self.attn = nn.Linear(hidden_units * 2, hidden_units)\n","        self.v = nn.Linear(hidden_units, 1, bias=False)\n","\n","    def forward(self, ht, encoder_out): \n","        ht = ht.repeat(encoder_out.shape[1], 1 , 1)\n","        ht = ht.transpose(0,1)\n","        energy = torch.cat([ht, encoder_out], dim=2)\n","        energy = self.attn(energy)\n","        energy = self.relu(energy)        \n","        attention = self.v(energy)\n","        attention = attention.squeeze(2)\n","        attention_weights = self.softmax(attention)\n","        attention_weights = attention_weights.unsqueeze(1)\n","        context = torch.bmm(attention_weights, encoder_out)\n","        return context, attention_weights\n","\n","class LSTMAttnDecoderBert(nn.Module):\n","    def __init__(self, input_size, embed_dim, hidden_units=1024, num_layers=1, p = 0.3, bidirectional=False):\n","        super(LSTMAttnDecoderBert, self).__init__()\n","        self.input_size = input_size\n","        self.embed_dim = embed_dim\n","        self.hidden_units = hidden_units\n","        self.num_layers = num_layers\n","        self.bidirectional = bidirectional\n","        self.dropout = nn.Dropout(p)\n","        self.embedding = nn.Embedding(input_size, self.embed_dim, padding_idx=0)\n","        self.attention = AttentionNetworkBert(hidden_units)\n","        self.LSTM = nn.LSTM(hidden_units + embed_dim, hidden_units, num_layers = num_layers, bidirectional = bidirectional, dropout=p, batch_first=True)\n","        self.fc = nn.Linear(hidden_units, input_size)\n","\n","    def forward(self, x, h0_c0, encoder_out):\n","        x = self.dropout(self.embedding(x))\n","        x = x.unsqueeze(1)\n","        context, attention_weights = self.attention(h0_c0[0], encoder_out)\n","        x = torch.cat([x, context], dim=2)\n","        decoder_out, (ht, ct) = self.LSTM(x, h0_c0)\n","        out = self.fc(decoder_out)\n","        \n","        return out, (ht, ct)"]},{"cell_type":"markdown","metadata":{},"source":["SEQ2SEQ"]},{"cell_type":"code","execution_count":26,"metadata":{"execution":{"iopub.execute_input":"2024-04-03T19:51:20.592723Z","iopub.status.busy":"2024-04-03T19:51:20.592346Z","iopub.status.idle":"2024-04-03T19:51:20.610662Z","shell.execute_reply":"2024-04-03T19:51:20.609758Z","shell.execute_reply.started":"2024-04-03T19:51:20.592693Z"},"trusted":true},"outputs":[],"source":["class Seq2Seq(nn.Module):\n","    def __init__(self,args):\n","        super(Seq2Seq, self).__init__()\n","        self.args=args\n","\n","        #extra see if needed \n","        self.model_type = args.model_type\n","        self.embed_dim = args.embed_dim        \n","        self.encoder_hidden_units = args.en_hidden\n","        self.decoder_hidden_units = args.de_hidden\n","        self.encoder_num_layers = args.en_num_layers\n","        self.decoder_num_layers = args.de_num_layers\n","        self.processed_data = args.processed_data\n","\n","        self.encoder_word2idx = self.get_encoder_word2idx()\n","        self.decoder_word2idx = self.get_decoder_word2idx()\n","        self.encoder_input_size = len(self.encoder_word2idx)\n","        self.decoder_input_size = len(self.decoder_word2idx)\n","        self.encoder = self.get_encoder()\n","        self.decoder = self.get_decoder()\n","\n","    def get_encoder_word2idx(self):\n","        with open(os.path.join(self.processed_data, \"encoder_word2idx.pickle\"), \"rb\") as file:\n","            word2idx = pickle.load(file)\n","        with open(os.path.join(self.processed_data, \"encoder_idx2word.pickle\"), \"rb\") as file:\n","            idx2word = pickle.load(file)\n","                \n","        \n","        return word2idx\n","    \n","    def get_decoder_word2idx(self):\n","        \n","        with open(os.path.join(self.processed_data, \"decoder_word2idx.pickle\"), \"rb\") as file:\n","            word2idx = pickle.load(file)\n","        with open(os.path.join(self.processed_data, \"decoder_idx2word.pickle\"), \"rb\") as file:\n","            idx2word = pickle.load(file)\n","        \n","        return word2idx\n","\n","    def get_encoder(self):\n","        print(\"Loading GloVe embeddings...\")\n","        glove = GloveEmbeddings(self.embed_dim, self.encoder_word2idx)\n","        embedding_matrix = glove.get_embedding_matrix()\n","        print(\"Loading Encoder...\")\n","        encoder = LSTMEncoder(input_size = self.encoder_input_size, embed_dim = self.embed_dim, \n","                              hidden_units=self.encoder_hidden_units, num_layers=self.encoder_num_layers, p = 0.3, embed_matrix=embedding_matrix).to(device)\n","        \n","        return encoder\n","    \n","    def get_decoder(self):\n","        print(\"Loading Seq2Seq LSTM Decoder...\")\n","        decoder = LSTMDecoder(input_size = self.decoder_input_size, embed_dim = self.embed_dim, \n","                          hidden_units=self.decoder_hidden_units, num_layers=self.decoder_num_layers, p = 0.3).to(device)\n","        return decoder\n","    \n","    def forward(self, problem, linear_formula, tf_ratio=0.6):\n","        batch_size = problem.shape[0]\n","        target_len = linear_formula.shape[1]\n","\n","        _ ,(hidden, cell) = self.encoder(problem)\n","        target_vocab_size = self.decoder_input_size\n","        outputs = torch.zeros(batch_size, target_len, target_vocab_size).to(device)\n","        x =  linear_formula[:,0]\n","        for t in range(1, target_len):\n","            output,( hidden, cell) = self.decoder(x,( hidden, cell))\n","            output = output.squeeze(1)\n","            outputs[:,t,:] = output\n","            x = output.argmax(dim=1)\n","            x = linear_formula[:,t] if random.random() < tf_ratio else x\n","            # x = output.argmax(1)\n","        return outputs"]},{"cell_type":"markdown","metadata":{},"source":["SEQ2SEQ ATTENTION"]},{"cell_type":"code","execution_count":8,"metadata":{"execution":{"iopub.execute_input":"2024-04-03T19:44:01.023240Z","iopub.status.busy":"2024-04-03T19:44:01.022948Z","iopub.status.idle":"2024-04-03T19:44:01.038103Z","shell.execute_reply":"2024-04-03T19:44:01.037315Z","shell.execute_reply.started":"2024-04-03T19:44:01.023208Z"},"trusted":true},"outputs":[],"source":["class Seq2SeqAttn(nn.Module):\n","    def __init__(self,args):\n","        super(Seq2SeqAttn, self).__init__()\n","        self.args = args\n","        self.model_type = args.model_type\n","        self.embed_dim = args.embed_dim\n","        self.encoder_hidden_units = args.en_hidden\n","        self.decoder_hidden_units = args.de_hidden\n","        self.encoder_num_layers = args.en_num_layers\n","        self.decoder_num_layers = args.de_num_layers\n","        self.processed_data = args.processed_data\n","\n","        self.encoder_word2idx = self.get_encoder_word2idx()\n","        self.decoder_word2idx = self.get_decoder_word2idx()\n","        self.encoder_input_size = len(self.encoder_word2idx)\n","        self.decoder_input_size = len(self.decoder_word2idx)\n","        self.encoder = self.get_encoder()\n","        self.decoder = self.get_decoder()\n","\n","        self.start_token = self.decoder_word2idx[\"<sos>\"]\n","        self.end_token = self.decoder_word2idx[\"<eos>\"]\n","\n","    def get_encoder_word2idx(self):\n","        with open(os.path.join(self.processed_data, \"encoder_word2idx.pickle\"), \"rb\") as file:\n","            word2idx = pickle.load(file)\n","        \n","        return word2idx\n","\n","    def get_decoder_word2idx(self):\n","        with open(os.path.join(self.processed_data, \"decoder_word2idx.pickle\"), \"rb\") as file:\n","            word2idx = pickle.load(file)        \n","        \n","        return word2idx\n","\n","    def get_encoder(self):\n","        print(\"Loading GloVe embeddings...\")\n","        glove = GloveEmbeddings(self.embed_dim, self.encoder_word2idx)\n","        embedding_matrix = glove.get_embedding_matrix()\n","        print(\"Loading Encoder...\")\n","        encoder = LSTMEncoder(input_size = self.encoder_input_size, embed_dim = self.embed_dim, \n","                              hidden_units=self.encoder_hidden_units, num_layers=self.encoder_num_layers, p = 0.3, embed_matrix=embedding_matrix)\n","        return encoder\n","\n","    def get_decoder(self):\n","        decoder = LSTMAttnDecoder(input_size = self.decoder_input_size, embed_dim = self.embed_dim, \n","                        hidden_units=self.decoder_hidden_units, num_layers=self.decoder_num_layers, p = 0.3)\n","        return decoder\n","\n","    def forward(self, problem, linear_formula, tf_ratio=0.6):\n","        batch_size = problem.shape[0]\n","        target_len = linear_formula.shape[1]\n","        \n","        encoder_out ,(hidden, cell) = self.encoder(problem)\n","        target_vocab_size = self.decoder_input_size\n","        outputs = torch.zeros(batch_size, target_len, target_vocab_size).to(device)\n","        x =  linear_formula[:,0]\n","        for t in range(1, target_len):\n","            output,( hidden, cell) = self.decoder(x,( hidden, cell), encoder_out)\n","            output = output.squeeze(1)\n","            outputs[:,t,:] = output\n","            x = output.argmax(dim=1)\n","            x = linear_formula[:,t] if random.random() < tf_ratio else x # teacher force ratio\n","        return outputs"]},{"cell_type":"markdown","metadata":{},"source":["BERT2SEQUENCE ATTENTION"]},{"cell_type":"code","execution_count":9,"metadata":{"execution":{"iopub.execute_input":"2024-04-03T19:44:01.039569Z","iopub.status.busy":"2024-04-03T19:44:01.039311Z","iopub.status.idle":"2024-04-03T19:44:01.055494Z","shell.execute_reply":"2024-04-03T19:44:01.054656Z","shell.execute_reply.started":"2024-04-03T19:44:01.039548Z"},"trusted":true},"outputs":[],"source":["class Bert2SeqAttn(nn.Module):\n","    def __init__(self, args):\n","        super(Bert2SeqAttn, self).__init__()\n","        self.args = args\n","        self.model_type = args.model_type\n","        self.embed_dim = args.embed_dim        \n","        self.encoder_hidden_units = args.en_hidden\n","        self.decoder_hidden_units = args.de_hidden\n","        self.encoder_num_layers = args.en_num_layers\n","        self.decoder_num_layers = args.de_num_layers\n","        self.processed_data = args.processed_data\n","        self.encoder_word2idx = self.get_encoder_word2idx()\n","        self.decoder_word2idx = self.get_decoder_word2idx()\n","        self.encoder_input_size = len(self.encoder_word2idx)\n","        self.decoder_input_size = len(self.decoder_word2idx)\n","        self.encoder = self.get_encoder()\n","        self.decoder = self.get_decoder()\n","        self.start_token = self.decoder_word2idx[\"<sos>\"]\n","        self.end_token = self.decoder_word2idx[\"<eos>\"]\n","\n","    def get_encoder_word2idx(self):\n","        with open(os.path.join(self.processed_data, \"encoder_word2idx.pickle\"), \"rb\") as file:\n","            word2idx = pickle.load(file)\n","        \n","        return word2idx\n","    \n","    def get_decoder_word2idx(self):\n","        \n","        with open(os.path.join(self.processed_data, \"decoder_word2idx.pickle\"), \"rb\") as file:\n","            word2idx = pickle.load(file)        \n","        \n","        return word2idx\n","\n","    def get_encoder(self):\n","        print(\"Loading Bert Encoder...\")\n","\n","        encoder = BertEncoder(self.model_type, self.args.bert_tune_layers, self.encoder_hidden_units)\n","        return encoder\n","\n","    def get_decoder(self):\n","        print(\"Loading Seq2Seq LSTM Attention Decoder...\")\n","        \n","        decoder = LSTMAttnDecoderBert(input_size = self.decoder_input_size, embed_dim = self.embed_dim, \n","                    hidden_units=self.decoder_hidden_units, num_layers=self.decoder_num_layers, p = 0.3)\n","            \n","        return decoder\n","\n","    def forward(self, problem, attn_mask, linear_formula, tf_ratio=0.6):\n","        batch_size = problem.shape[0]\n","        max_target_len = linear_formula.shape[1]\n","        \n","        encoder_out = self.encoder(problem, attn_mask)\n","\n","        target_vocab_size = self.decoder_input_size\n","        outputs = torch.zeros(batch_size, max_target_len, target_vocab_size).to(device)\n","        words = torch.zeros(batch_size, max_target_len).to(device)\n","        \n","        hidden = torch.zeros(1, batch_size, self.decoder_hidden_units).to(device)\n","        cell = torch.zeros(1, batch_size, self.decoder_hidden_units).to(device)\n","        x = linear_formula[:,0]            \n","        words[:, 0] = linear_formula[:,0]\n","        for t in range(1, max_target_len):\n","            # print(\"DECODER INPUT SHAPES x.shape, hidden.shape, cell.shape, encoder_out.shape\", x.shape, hidden.shape, cell.shape, encoder_out.shape)\n","            output, (hidden, cell) = self.decoder(x, (hidden, cell), encoder_out)\n","#             print(\"Seq2seq out shape\", output.shape)\n","            output = output.squeeze(1)\n","            outputs[:,t,:] = output\n","            x = output.argmax(dim=1)\n","            x = linear_formula[:,t] if random.random() < tf_ratio else x\n","            words[:, t] = x\n","        return outputs, words"]},{"cell_type":"markdown","metadata":{},"source":["DATASET"]},{"cell_type":"markdown","metadata":{},"source":["TEXTTOMATH"]},{"cell_type":"code","execution_count":10,"metadata":{"execution":{"iopub.execute_input":"2024-04-03T19:44:01.057298Z","iopub.status.busy":"2024-04-03T19:44:01.056764Z","iopub.status.idle":"2024-04-03T19:44:01.076651Z","shell.execute_reply":"2024-04-03T19:44:01.075663Z","shell.execute_reply.started":"2024-04-03T19:44:01.057253Z"},"trusted":true},"outputs":[],"source":["class TextToMathDataset(Dataset):\n","    def __init__(self, file_path, data_prefix = \"train\"):\n","        self.file_path = file_path\n","        pth = os.path.join(self.file_path, f\"{data_prefix}.json\")\n","        \n","        print(pth)\n","        self.data  = json.load(open(pth))\n","        self.data = self.data[:100]\n","        print(\"Dataset Length =\", len(self.data))\n","#         print(\"Dataset Length =\", len(self.data))\n","\n","        with open(os.path.join(file_path, \"encoder.vocab\"), \"r\") as file:\n","            vocab = file.readlines()\n","        self.encoder_vocab = vocab\n","        \n","        with open(os.path.join(file_path, \"decoder.vocab\"), \"r\") as file:\n","            vocab = file.readlines()\n","        self.decoder_vocab = vocab\n","\n","        with open(os.path.join(file_path, \"encoder_word2idx.pickle\"), \"rb\") as file:\n","            word2idx = pickle.load(file)\n","        with open(os.path.join(file_path, \"encoder_idx2word.pickle\"), \"rb\") as file:\n","            idx2word = pickle.load(file)\n","\n","        self.en_word2idx = word2idx\n","        self.en_idx2word = idx2word\n","\n","        with open(os.path.join(file_path, \"decoder_word2idx.pickle\"), \"rb\") as file:\n","            word2idx = pickle.load(file)\n","        with open(os.path.join(file_path, \"decoder_idx2word.pickle\"), \"rb\") as file:\n","            idx2word = pickle.load(file)\n","            \n","        self.de_word2idx = word2idx\n","        self.de_idx2word = idx2word\n","\n","        print(\"Encoder Vocab Size = {}, Decoder Vocab Size = {}\".format(len(self.en_word2idx), len(self.de_word2idx)))\n","        print(\"Encoder word2idx Size = {}, Decoder word2idx Size = {}\".format(len(self.en_word2idx), len(self.de_word2idx)))\n","\n","    def __len__(self):  \n","        return len(self.data)\n","    \n","\n","    def __getitem__(self, index):\n","        problem = self.data[index][\"Problem\"]\n","        problem = [\"<sos>\"] + tokenize_problem(problem) + [\"<eos>\"]\n","\n","        linear_formula = self.data[index][\"linear_formula\"]\n","        formula_split = [\"<sos>\"] +tokenize_formula(linear_formula) + [\"<eos>\"]\n","        \n","        aa = self.data[index][\"answer\"]\n","        problem = [self.en_word2idx[q] if q in self.en_word2idx else self.en_word2idx[\"<unk>\"] for q in problem]\n","        formula_split = [self.de_word2idx[q] if q in self.de_word2idx else self.de_word2idx[\"<unk>\"] for q in formula_split]\n","        \n","        answer = {\"Problem\" : problem , \"linear_formula\" : formula_split, \"answer\" : aa }\n","        return answer\n","    \n","\n","def collate(batch):\n","    max_len_problem = max([len(sample['Problem']) for sample in batch])\n","    max_len_formula = max([len(sample['linear_formula']) for sample in batch])\n","\n","    \n","    problem_lens = torch.zeros(len(batch), dtype=torch.long)\n","    padded_problem = torch.zeros((len(batch), max_len_problem), dtype=torch.long)\n","    \n","    formula_lens = torch.zeros(len(batch), dtype=torch.long)\n","    padded_formula = torch.zeros((len(batch), max_len_formula), dtype=torch.long)\n","\n","    answers = torch.zeros(len(batch), dtype=torch.long)\n","    for idx in range(len(batch)):\n","        problem = batch[idx]['Problem']\n","        linear_formula = batch[idx]['linear_formula']\n","        ans = batch[idx]['answer']\n","        prob_len = len(problem)\n","        lf_len = len(linear_formula)\n","        problem_lens[idx] = prob_len\n","        formula_lens[idx] = lf_len\n","        problem_tensor = torch.LongTensor(problem)\n","        linear_formula_tensor = torch.LongTensor(linear_formula)\n","        padded_problem[idx, :prob_len] = problem_tensor\n","        padded_formula[idx, :lf_len] = linear_formula_tensor\n","        answers[idx] = ans\n","        \n","    ret = {'problem': padded_problem, 'problem_lens': problem_lens, 'linear_formula': padded_formula, 'formula_lens': formula_lens, 'answer' : answers}\n","\n","    return ret\n"]},{"cell_type":"markdown","metadata":{},"source":["TEXT2MATHBERT"]},{"cell_type":"code","execution_count":11,"metadata":{"execution":{"iopub.execute_input":"2024-04-03T19:44:01.078259Z","iopub.status.busy":"2024-04-03T19:44:01.077947Z","iopub.status.idle":"2024-04-03T19:44:01.096251Z","shell.execute_reply":"2024-04-03T19:44:01.095378Z","shell.execute_reply.started":"2024-04-03T19:44:01.078235Z"},"trusted":true},"outputs":[],"source":["class Text2MathBertDataset(Dataset):\n","    def __init__(self, file_path, data_prefix = \"train\"):\n","        self.file_path = file_path\n","        pth = os.path.join(self.file_path, f\"{data_prefix}.json\")\n","        \n","        print(pth)\n","        self.data  = json.load(open(pth))\n","        self.data = self.data[:100]\n","        print(\"Dataset Length =\", len(self.data))\n","        \n","        with open(os.path.join(file_path, \"decoder.vocab\"), \"r\") as file:\n","            vocab = file.readlines()\n","        self.decoder_vocab = vocab\n","        \n","        with open(os.path.join(file_path, \"decoder_word2idx.pickle\"), \"rb\") as file:\n","            word2idx = pickle.load(file)\n","        with open(os.path.join(file_path, \"decoder_idx2word.pickle\"), \"rb\") as file:\n","            idx2word = pickle.load(file)\n","            \n","        self.de_word2idx = word2idx\n","        self.de_idx2word = idx2word\n","\n","        self.en_tokenizer =  BertTokenizer.from_pretrained(\"bert-base-cased\")\n","\n","        print(\"Encoder Vocab Size = , Decoder Vocab Size = {}\".format( len(self.de_word2idx)))\n","        \n","    def __len__(self):        \n","        return len(self.data)\n","    \n","    def __getitem__(self, index):\n","        problem = self.data[index][\"Problem\"]\n","        problem = self.en_tokenizer.encode(problem)\n","        \n","        linear_formula = self.data[index][\"linear_formula\"]\n","        formula_split = [\"<sos>\"] +tokenize_formula(linear_formula) + [\"<eos>\"]\n","        formula_split = [self.de_word2idx[q] if q in self.de_word2idx else self.de_word2idx[\"<unk>\"] for q in formula_split]\n","        aa = self.data[index][\"answer\"]\n","\n","        answer = {\"Problem\" : problem , \"linear_formula\" : formula_split, \"answer\" : aa }\n","            \n","        return answer\n","\n","def collate_bert(batch):\n","    \n","    max_len_problem = max([len(sample['Problem']) for sample in batch])\n","    max_len_formula = max([len(sample['linear_formula']) for sample in batch])\n","    \n","    problem_lens = torch.zeros(len(batch), dtype=torch.long)\n","    padded_problem = torch.zeros((len(batch), max_len_problem), dtype=torch.long)\n","    problem_attn_mask = torch.zeros((len(batch), max_len_problem), dtype=torch.long)\n","    \n","    formula_lens = torch.zeros(len(batch), dtype=torch.long)\n","    padded_formula = torch.zeros((len(batch), max_len_formula), dtype=torch.long)\n","\n","    answers = torch.zeros(len(batch), dtype=torch.long)\n","    for idx in range(len(batch)):\n","        \n","        problem = batch[idx]['Problem']\n","        linear_formula = batch[idx]['linear_formula']\n","        \n","        prob_len = len(problem)\n","        lf_len = len(linear_formula)\n","        problem_lens[idx] = prob_len\n","        formula_lens[idx] = lf_len\n","\n","        \n","        padded_problem[idx, :prob_len] = torch.LongTensor(problem)\n","        problem_attn_mask[idx, :prob_len] = torch.ones((1, prob_len), dtype=torch.long)\n","\n","        padded_formula[idx, :lf_len] = torch.LongTensor(linear_formula)\n","\n","        answers[idx] = batch[idx]['answer']\n","        \n","    ret = {'problem': padded_problem, 'problem_lens': problem_lens, 'problem_attn_mask': problem_attn_mask, 'linear_formula': padded_formula, 'formula_lens': formula_lens, 'answer' : answers}\n","\n","    return ret\n"]},{"cell_type":"markdown","metadata":{},"source":["BEAM SEARCH"]},{"cell_type":"code","execution_count":12,"metadata":{"execution":{"iopub.execute_input":"2024-04-03T19:44:01.097725Z","iopub.status.busy":"2024-04-03T19:44:01.097424Z","iopub.status.idle":"2024-04-03T19:44:01.110009Z","shell.execute_reply":"2024-04-03T19:44:01.109138Z","shell.execute_reply.started":"2024-04-03T19:44:01.097702Z"},"trusted":true},"outputs":[],"source":["def beam_search(args, model, en_ht, en_ct, start_token, end_token, max_target_len = 80, beam_size = 10):\n","    beam = [([start_token], (en_ht, en_ct), 0)]\n","#     print(\"Beam Search input hidden and cell shape, start and end tokens\", en_ht.shape, en_ct.shape, start_token, end_token)\n","\n","    i = 0\n","    while i < max_target_len -1:\n","#         print(\"i : \" , i)\n","        new_beam = []\n","        for sequence, (ht, ct), score in beam:\n","            prev_token = [sequence[-1]] #get first token for each beam\n","            prev_token = torch.LongTensor(prev_token).to(device)\n","\n","            decoder_out, (ht, ct) = model.decoder(prev_token, (ht, ct)) #pass through decoder\n","\n","            decoder_out = decoder_out.squeeze(1)\n","            top_info = decoder_out.topk(beam_size, dim=1) #get top k=beam_size possible word indices and their values\n","            top_vals, top_inds = top_info\n","\n","            for j in range(beam_size):\n","                new_word_idx = top_inds[0][j]                \n","                new_seq = sequence + [new_word_idx.item()]\n","                new_word_prob = torch.log(top_vals[0][j])\n","                updated_score = score - new_word_prob\n","                new_candidate = (new_seq, (ht, ct), updated_score)\n","                new_beam.append(new_candidate)\n","\n","        # new_beam = sorted(new_beam, reverse=False, key=lambda x: x[2])\n","        new_beam.sort(key=lambda x: x[2])\n","        beam = new_beam[:beam_size]\n","        i += 1\n","        \n","\n","    best_candidate = beam[0][0] #return best candidate based on score\n","    decoded_words = torch.zeros(1, max_target_len)\n","\n","    for t in range(max_target_len):\n","        decoded_words[:, t] = torch.LongTensor([best_candidate[t]])\n","    \n","    return decoded_words"]},{"cell_type":"code","execution_count":13,"metadata":{"execution":{"iopub.execute_input":"2024-04-03T19:44:01.111362Z","iopub.status.busy":"2024-04-03T19:44:01.111104Z","iopub.status.idle":"2024-04-03T19:44:01.125283Z","shell.execute_reply":"2024-04-03T19:44:01.124372Z","shell.execute_reply.started":"2024-04-03T19:44:01.111341Z"},"trusted":true},"outputs":[],"source":["def beam_search_attn_decoder(args, model, encoder_out, en_ht, en_ct, start_token, end_token, max_target_len = 80, beam_size = 3):\n","    beam = [([start_token], (en_ht, en_ct), 0)]\n","    # print(\"Beam Search input hidden and cell shape, start and end tokens\", en_ht.shape, en_ct.shape, start_token, end_token)\n","\n","    i = 0\n","    while i < max_target_len -1:\n","        new_beam = []\n","        for sequence, (ht, ct), score in beam:\n","            prev_token = [sequence[-1]] #get first token for each beam\n","            prev_token = torch.LongTensor(prev_token).to(device)\n","\n","            decoder_out, (ht, ct) = model.decoder(prev_token, (ht, ct), encoder_out) #pass through decoder\n","\n","            decoder_out = decoder_out.squeeze(1)\n","            top_info = decoder_out.topk(beam_size, dim=1) #get top k=beam_size possible word indices and their values\n","            top_vals, top_inds = top_info\n","\n","            for j in range(beam_size):\n","                new_word_idx = top_inds[0][j]                \n","                new_seq = sequence + [new_word_idx.item()]\n","                new_word_prob = torch.log(top_vals[0][j])\n","                updated_score = score - new_word_prob\n","                new_candidate = (new_seq, (ht, ct), updated_score)\n","                new_beam.append(new_candidate)\n","\n","        # new_beam = sorted(new_beam, reverse=True, key=lambda x: x[2])\n","        new_beam.sort(key=lambda x: x[2])\n","        beam = new_beam[:beam_size]\n","        i += 1\n","\n","    best_candidate = beam[0][0] #return best candidate based on score\n","    decoded_words = torch.zeros(1, max_target_len)\n","\n","    for t in range(max_target_len):\n","        decoded_words[:, t] = torch.LongTensor([best_candidate[t]])\n","    \n","    return decoded_words"]},{"cell_type":"markdown","metadata":{},"source":["HELPER FUNCTIONS"]},{"cell_type":"code","execution_count":14,"metadata":{"execution":{"iopub.execute_input":"2024-04-03T19:44:01.129450Z","iopub.status.busy":"2024-04-03T19:44:01.129152Z","iopub.status.idle":"2024-04-03T19:44:01.141382Z","shell.execute_reply":"2024-04-03T19:44:01.140385Z","shell.execute_reply.started":"2024-04-03T19:44:01.129427Z"},"trusted":true},"outputs":[],"source":["def convert_idx_sentence(args, output, problem, linear_formula, answer, de_idx2word, en_idx2word, mode):\n","    \"\"\"\n","    write to a file named my_output.json\n","    format for all problem should be\n","    {\n","        \"Problem\" : <problem>\n","        \"answer\" : <answer>\n","        \"predicted\" : <predicted_output>\n","        \"linear_formula\" : <original linear formula>\n","    }\n","    \"\"\"\n","    output = output.cpu().detach().numpy()\n","    linear_formula = list(linear_formula)\n","    problem = list(problem)\n","    batch_size = output.shape[0]\n","    if(mode == \"dev\"):\n","        output_file = os.path.join(\"/kaggle/working/\",f\"dev_output.json\")\n","    elif (mode == \"test\"):\n","        output_file = os.path.join(\"/kaggle/working/\",f\"test_output.json\")\n","    else:\n","        print(\"Mode not recognized\")\n","        return \n","    # assert len(output) == len(linear_formula)\n","\n","    # convert all the tensors into words usinf idx2word dictionary\n","    for b in range(batch_size):\n","        prb = \"\"\n","        ans = \"\"\n","        predicted = \"\"\n","        lf = \"\"\n","        for i in range(len(output[b])):\n","#             if(mode == \"test\"):\n","#                 print(\"Size : \",  len(de_idx2word))\n","#                 print(\"output[b][i] : \" , output[b][i])\n","#                 print(\"temp \",de_idx2word[(int)(output[b][i])] )\n","            temp = de_idx2word[(int)(output[b][i])]\n","            predicted += temp\n","            if(temp ==\"<eos>\"):\n","                break\n","\n","        for i in range(len(linear_formula[b])):\n","            temp = de_idx2word[(int)(linear_formula[b][i].item())]\n","            lf += temp \n","            if(temp ==\"<eos>\"):\n","                break\n","\n","        for i in range(len(problem[b])):\n","            temp= en_idx2word[(int)(problem[b][i].item())]\n","            prb += temp + \" \"\n","            if(temp == \"<eos>\"):\n","                break\n","\n","        \n","        ans += str(answer[b].item())\n","\n","        data = {\n","            \"Problem\" : prb,\n","            \"answer\" : ans,\n","            \"predicted\" : predicted,\n","            \"linear_formula\" : lf\n","        }\n","\n","        with open(output_file, \"a\") as file:\n","            json.dump(data, file)\n","            file.write(\"\\n\")\n","    return\n"]},{"cell_type":"markdown","metadata":{},"source":["INFER"]},{"cell_type":"code","execution_count":15,"metadata":{"execution":{"iopub.execute_input":"2024-04-03T19:44:01.142731Z","iopub.status.busy":"2024-04-03T19:44:01.142455Z","iopub.status.idle":"2024-04-03T19:44:01.154860Z","shell.execute_reply":"2024-04-03T19:44:01.154005Z","shell.execute_reply.started":"2024-04-03T19:44:01.142709Z"},"trusted":true},"outputs":[],"source":["def evaluator_dev(args, model):\n","    # on current model given , need to modify for any model\n","    dev_dataset = TextToMathDataset(args.processed_data, \"dev\")\n","    dev_loader = DataLoader(dev_dataset, batch_size=args.batch_size, shuffle=False, collate_fn=collate, num_workers=args.num_workers)\n","    de_word2idx = dev_dataset.de_word2idx\n","    de_idx2word = dev_dataset.de_idx2word\n","    en_idx2word = dev_dataset.en_idx2word\n","\n","    model.eval()\n","\n","    print(\"Evaluating model on val data on given model\")\n","\n","    start_token = de_word2idx[\"<sos>\"]\n","    end_token = de_word2idx[\"<eos>\"]\n","\n","    target_vocab_size = len(de_word2idx)\n","    decoder_hidden_units = args.de_hidden\n","\n","    #file to be made\n","    my_file = \"my_output.json\"\n","    my_file = os.path.join(args.processed_data, my_file)\n","\n","    for i, batch in enumerate(dev_loader):\n","        problem = batch['problem'].to(device)\n","        linear_formula = batch['linear_formula'].to(device)\n","        answer = batch['answer'].to(device)\n","        \n","        batch_size = problem.shape[0]\n","        max_target_len = linear_formula.shape[1]\n","        # max_target_len = 500\n","\n","        words = torch.zeros(batch_size, max_target_len).to(device)\n","\n","        output , (hidden, cell)  = model.encoder(problem)\n","\n","        #beam search\n","        for b in range(batch_size):\n","#             print(f\"at i: {i} and inside batch :{b}\")\n","            words[b,:] = beam_search(args, model , hidden[:,b,:].unsqueeze(1), cell[:,b,:].unsqueeze(1), start_token, end_token, max_target_len = max_target_len, beam_size = 10)\n","        # convert_idx_sentence(args, words, problem, linear_formula, answer)\n","        convert_idx_sentence(args, words, problem, linear_formula, answer, de_idx2word, en_idx2word, \"dev\")\n","    return"]},{"cell_type":"markdown","metadata":{},"source":["TRAIN"]},{"cell_type":"code","execution_count":16,"metadata":{"execution":{"iopub.execute_input":"2024-04-03T19:44:01.156282Z","iopub.status.busy":"2024-04-03T19:44:01.155997Z","iopub.status.idle":"2024-04-03T19:44:01.169373Z","shell.execute_reply":"2024-04-03T19:44:01.168325Z","shell.execute_reply.started":"2024-04-03T19:44:01.156260Z"},"trusted":true},"outputs":[],"source":["def evaluator_dev_attn(args, model):\n","    # on current model given , need to modify for any model\n","    dev_dataset = TextToMathDataset(args.processed_data, \"dev\")\n","    dev_loader = DataLoader(dev_dataset, batch_size=args.batch_size, shuffle=True, collate_fn=collate, num_workers=args.num_workers)\n","    de_word2idx = dev_dataset.de_word2idx\n","    de_idx2word = dev_dataset.de_idx2word\n","    en_idx2word = dev_dataset.en_idx2word\n","\n","    model.eval()\n","\n","    print(\"Evaluating model on val data on given model\")\n","\n","    start_token = de_word2idx[\"<sos>\"]\n","    end_token = de_word2idx[\"<eos>\"]\n","\n","    target_vocab_size = len(de_word2idx)\n","    decoder_hidden_units = args.de_hidden\n","\n","    #file to be made\n","    my_file = \"my_output.json\"\n","    my_file = os.path.join(args.processed_data, my_file)\n","\n","    for i, batch in enumerate(dev_loader):\n","        problem = batch['problem'].to(device)\n","        linear_formula = batch['linear_formula'].to(device)\n","        answer = batch['answer'].to(device)\n","\n","        batch_size = problem.shape[0]\n","        max_target_len = linear_formula.shape[1]\n","        #max_target_len = 500\n","\n","        words = torch.zeros(batch_size, max_target_len).to(device)\n","        output, (hidden,cell)= model.encoder(problem)\n","    \n","        #beam search\n","        for b in range(batch_size):\n","#             print(f\" i : {i} , b : {b}\")\n","            words[b,:] = beam_search_attn_decoder(args, model , output[b,:,:].unsqueeze(0), hidden[:,b,:].unsqueeze(1), cell[:,b,:].unsqueeze(1), start_token, end_token, max_target_len = max_target_len, beam_size = 10)\n","        convert_idx_sentence(args, words, problem, linear_formula, answer, de_idx2word, en_idx2word, \"test\")\n","    # print(\"Running evaluation script for test ... \")\n","    return"]},{"cell_type":"markdown","metadata":{},"source":["TRAIN AND DEV"]},{"cell_type":"code","execution_count":17,"metadata":{"execution":{"iopub.execute_input":"2024-04-03T19:44:01.171125Z","iopub.status.busy":"2024-04-03T19:44:01.170624Z","iopub.status.idle":"2024-04-03T19:44:01.191439Z","shell.execute_reply":"2024-04-03T19:44:01.190514Z","shell.execute_reply.started":"2024-04-03T19:44:01.171096Z"},"trusted":true},"outputs":[],"source":["def train_S2S():\n","    print(device)\n","    args = ARGS()\n","    pth = \"/kaggle/input/math-data\"\n","    args.processed_data = os.path.join(pth, \"processed_data\")\n","    args.data_dir = os.path.join(pth, \"data\")\n","    print(args.processed_data)\n","    train_dataset = TextToMathDataset(args.processed_data, \"train\")\n","    dev_dataset = TextToMathDataset(args.processed_data, \"dev\")\n","    train_loader = DataLoader(train_dataset, batch_size = args.batch_size, shuffle = True, num_workers = args.num_workers, collate_fn=collate)\n","    dev_loader = DataLoader(dev_dataset, batch_size = args.batch_size, shuffle = False, num_workers = args.num_workers, collate_fn=collate)\n","\n","    #------------------------------------------------\n","    model = Seq2Seq(args).to(device)\n","\n","    criterion = nn.CrossEntropyLoss()\n","    optimizer = torch.optim.Adam(model.parameters(), lr = 0.001) \n","#     schedulers = {\n","#         \"stepLR\" : torch.optim.lr_scheduler.StepLR(optimizer, step_size=20, gamma=0.1, last_epoch=- 1, verbose=False),\n","#         \"cosineLR\" : torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, args.epochs, verbose=False)\n","#     }\n","\n","#     current_scheduler = schedulers[\"cosineLR\"]\n","    # ----------------------------------\n","    loss_tracker = defaultdict(list)\n","    time_tracker = defaultdict(list)\n","    # val_accuracy_tracker = defaultdict(list)\n","    min_loss = 1000000\n","    best_epoch = 0\n","    start = time()\n","\n","    for epoch in range(args.epochs):\n","        print(\"\\n\\n-------------------Epoch = \", epoch, \"------------------------------\\n\")\n","        model.train()\n","        epoch_loss =[]\n","        total_loss = 0\n","        for i, batch in enumerate(train_loader):\n","            optimizer.zero_grad()\n","\n","            prb = batch[\"problem\"].to(device)\n","            lf = batch[\"linear_formula\"].to(device)\n","\n","            output = model(prb, lf)\n","\n","            output = output.reshape(-1, output.shape[2])\n","            lf = lf.reshape(-1)\n","            \n","            loss = criterion(output, lf)\n","            loss.backward()\n","            epoch_loss.append(loss.item())\n","            total_loss += loss.item()\n","            optimizer.step()\n","\n","        end1 = time()\n","        print(f\"Epoch {epoch}, Loss = {total_loss/len(train_loader)}\")\n","\n","        dev_loss =[]\n","        model.eval()\n","        for i, batch in enumerate(dev_loader):\n","            prb = batch[\"problem\"].to(device)\n","            lf = batch[\"linear_formula\"].to(device)\n","\n","            output  = model(prb, lf)\n","\n","            output = output.reshape(-1, output.shape[2])\n","            lf = lf.reshape(-1)\n","\n","            loss = criterion(output, lf)\n","            dev_loss.append(loss.item())\n","\n","\n","        end2 = time()\n","        avg_dev_loss = np.mean(dev_loss)\n","        avg_epoch_loss = np.mean(epoch_loss)\n","\n","        loss_tracker[\"train\"].append(avg_epoch_loss)\n","        loss_tracker[\"dev\"].append(avg_dev_loss)\n","\n","        print(f\"Epoch {epoch}, Train Loss = {avg_epoch_loss}, Dev Loss = {avg_dev_loss}\")\n","        # print(f\"Epoch Time = {end2 - end1}\")\n","        time_tracker[\"train\"].append(round((end1 - start)/60 , 2))\n","        # loss_tracker[\"dev\"].append(avg_dev_loss)\n","\n","        with open(os.path.join(args.result_dir, \"loss_tracker{}.json\".format(args.model_type)), \"w\") as outfile:\n","            json.dump(loss_tracker, outfile)\n","\n","        torch.save(model, os.path.join(args.checkpoint_dir, \"latest_checkpoint_{}.pth\".format(args.model_type)))\n","\n","        model_state = {\n","                'epoch': epoch,\n","                'train_loss' : avg_epoch_loss,\n","                'prev_best_epoch': best_epoch\n","        }\n","\n","        with open(os.path.join(args.checkpoint_dir, \"latest_chkpt_status_{}.json\".format(args.model_type)), \"w\") as outfile:\n","            json.dump(model_state, outfile)\n","\n","        #save the model whose loss is minimum\n","        if avg_dev_loss < min_loss:\n","            min_loss = avg_dev_loss\n","            best_epoch = epoch\n","            torch.save(model, os.path.join(args.checkpoint_dir, \"best_checkpoint_{}.pth\".format(args.model_type)))\n","            print(\"Best Model saved at epoch = \", epoch)\n","\n","    print(\"Training and Dev Complete for Seq2Seq model\")\n","    print(\"Dev evaluater ...\")\n","    \n","    #all epochs done\n","    #load the best model and return\n","    best_model = torch.load(os.path.join(args.checkpoint_dir, \"best_checkpoint_{}.pth\".format(args.model_type)))\n","#     evaluator_dev(args, best_model)\n","    print(\"Dev test\")\n","    evaluator_dev(args, best_model)\n","    return args , best_model\n","    "]},{"cell_type":"code","execution_count":18,"metadata":{"execution":{"iopub.execute_input":"2024-04-03T19:44:01.193031Z","iopub.status.busy":"2024-04-03T19:44:01.192717Z","iopub.status.idle":"2024-04-03T19:44:01.214827Z","shell.execute_reply":"2024-04-03T19:44:01.213896Z","shell.execute_reply.started":"2024-04-03T19:44:01.193004Z"},"trusted":true},"outputs":[],"source":["def train_S2SAttn():\n","    print(device)\n","    args = ARGS()\n","    pth = \"/kaggle/input/math-data\"\n","    args.processed_data = os.path.join(pth, \"processed_data\")\n","    args.data_dir = os.path.join(pth, \"data\")\n","    print(args.processed_data)\n","\n","    train_dataset = TextToMathDataset(args.processed_data, \"train\")\n","    dev_dataset = TextToMathDataset(args.processed_data, \"dev\")\n","    train_loader = DataLoader(train_dataset, batch_size = args.batch_size, shuffle = True, num_workers = args.num_workers, collate_fn=collate)\n","    dev_loader = DataLoader(dev_dataset, batch_size = args.batch_size, shuffle = False, num_workers = args.num_workers, collate_fn=collate)\n","\n","    #------------------------------------------------\n","    model = Seq2SeqAttn(args).to(device)\n","\n","    criterion = nn.CrossEntropyLoss(ignore_index = 0)\n","    optimizer = torch.optim.Adam(model.parameters(), lr = 0.0001)  #adam or SGD ??\n","    schedulers = {\n","        \"stepLR\" : torch.optim.lr_scheduler.StepLR(optimizer, step_size=20, gamma=0.1, last_epoch=- 1, verbose=False),\n","        \"cosineLR\" : torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, args.epochs, verbose=False)\n","    }\n","\n","    current_scheduler = schedulers[\"cosineLR\"]\n","    # ----------------------------------\n","    loss_tracker = defaultdict(list)\n","    time_tracker = defaultdict(list)\n","    # val_accuracy_tracker = defaultdict(list)\n","    min_loss = 1000000\n","    best_epoch = 0\n","    start = time()\n","\n","    for epoch in range(args.epochs):\n","        print(\"\\n\\n-------------------Epoch = \", epoch, \"------------------------------\\n\")\n","        model.train()\n","        epoch_loss =[]\n","        total_loss = 0\n","        for i, batch in enumerate(train_loader):\n","            optimizer.zero_grad()\n","\n","            prb = batch[\"problem\"].to(device)\n","            lf = batch[\"linear_formula\"].to(device)\n","\n","            output = model(prb, lf)\n","\n","            output = output.reshape(-1, output.shape[2])\n","            lf = lf.reshape(-1)\n","            \n","            loss = criterion(output, lf)\n","            loss.backward()\n","            epoch_loss.append(loss.item())\n","            total_loss += loss.item()\n","            optimizer.step()\n","\n","        current_scheduler.step()\n","        end1 = time()\n","        print(f\"Epoch {epoch}, Loss = {total_loss/len(train_loader)}\")\n","\n","        dev_loss =[]\n","        model.eval()\n","        for i, batch in enumerate(dev_loader):\n","            prb = batch[\"problem\"].to(device)\n","            lf = batch[\"linear_formula\"].to(device)\n","\n","            output = model(prb, lf)\n","\n","            output = output.reshape(-1, output.shape[2])\n","            lf = lf.reshape(-1)\n","\n","            loss = criterion(output, lf)\n","            dev_loss.append(loss.item())\n","\n","\n","        end2 = time()\n","        avg_dev_loss = np.mean(dev_loss)\n","        avg_epoch_loss = np.mean(epoch_loss)\n","\n","        loss_tracker[\"train\"].append(avg_epoch_loss)\n","        loss_tracker[\"dev\"].append(avg_dev_loss)\n","\n","        print(f\"Epoch {epoch}, Train Loss = {avg_epoch_loss}, Dev Loss = {avg_dev_loss}\")\n","        # print(f\"Epoch Time = {end2 - end1}\")\n","        time_tracker[\"train\"].append(round((end1 - start)/60 , 2))\n","        with open(os.path.join(args.result_dir, \"loss_tracker{}.json\".format(args.model_type)), \"w\") as outfile:\n","            json.dump(loss_tracker, outfile)\n","\n","        torch.save(model, os.path.join(args.checkpoint_dir, \"latest_checkpoint_{}.pth\".format(args.model_type)))\n","\n","        model_state = {\n","                'epoch': epoch,\n","                'train_loss' : avg_epoch_loss,\n","                'prev_best_epoch': best_epoch\n","        }\n","\n","        with open(os.path.join(args.checkpoint_dir, \"latest_chkpt_status_{}.json\".format(args.model_type)), \"w\") as outfile:\n","            json.dump(model_state, outfile)\n","\n","        #save the model whose loss is minimum\n","        if avg_dev_loss < min_loss:\n","            min_loss = avg_dev_loss\n","            best_epoch = epoch\n","            torch.save(model, os.path.join(args.checkpoint_dir, \"best_checkpoint_{}.pth\".format(args.model_type)))\n","            print(\"Best Model saved at epoch = \", epoch)\n","\n","    print(\"Training and Dev Complete for Seq2Seq Attn model\")\n","    #all epochs done\n","    #load the best model and return\n","    best_model = torch.load(os.path.join(args.checkpoint_dir, \"best_checkpoint_{}.pth\".format(args.model_type)))\n","    evaluator_dev(args,best_model)\n","    return args, best_model\n"]},{"cell_type":"code","execution_count":19,"metadata":{"execution":{"iopub.execute_input":"2024-04-03T19:44:01.216655Z","iopub.status.busy":"2024-04-03T19:44:01.216295Z","iopub.status.idle":"2024-04-03T19:44:01.236449Z","shell.execute_reply":"2024-04-03T19:44:01.235663Z","shell.execute_reply.started":"2024-04-03T19:44:01.216626Z"},"trusted":true},"outputs":[],"source":["def train_BertS2SAtten():\n","    print(device)\n","    args = ARGS()\n","    pth = \"/kaggle/input/math-data\"\n","    args.processed_data = os.path.join(pth, \"processed_data\")\n","    args.data_dir = os.path.join(pth, \"data\")\n","    print(args.processed_data)\n","    # build_vocab(args.processed_data)\n","    train_dataset = Text2MathBertDataset(args.processed_data, \"train\")\n","    dev_dataset = Text2MathBertDataset(args.processed_data, \"dev\")\n","    train_loader = DataLoader(train_dataset, batch_size = args.batch_size, shuffle = True, num_workers = args.num_workers, collate_fn=collate)\n","    dev_loader = DataLoader(dev_dataset, batch_size = args.batch_size, shuffle = False, num_workers = args.num_workers, collate_fn=collate)\n","\n","    #------------------------------------------------\n","    model = Bert2SeqAttn(args).to(device)\n","\n","    criterion = nn.CrossEntropyLoss(ignore_index = 0)\n","    optimizer = torch.optim.Adam(model.parameters(), lr = 0.001)  #adam or SGD ??\n","#     schedulers = {\n","#         \"stepLR\" : torch.optim.lr_scheduler.StepLR(optimizer, step_size=20, gamma=0.1, last_epoch=- 1, verbose=False),\n","#         \"cosineLR\" : torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, args.epochs, verbose=False)\n","#     }\n","\n","#     current_scheduler = schedulers[\"cosineLR\"]\n","    # ----------------------------------\n","    loss_tracker = defaultdict(list)\n","    time_tracker = defaultdict(list)\n","    # val_accuracy_tracker = defaultdict(list)\n","    min_loss = 1000000\n","    best_epoch = 0\n","    start = time()\n","\n","    for epoch in range(args.epochs):\n","        print(\"\\n\\n-------------------Epoch = \", epoch, \"------------------------------\\n\")\n","        model.train()\n","        epoch_loss =[]\n","        total_loss = 0\n","        for i, batch in enumerate(train_loader):\n","            optimizer.zero_grad()\n","\n","            prb = batch[\"problem\"].to(device)\n","            lf = batch[\"linear_formula\"].to(device)\n","            attn_mask = batch[\"problem_attn_mask\"].to(device)\n","\n","            output, _ = model(prb, attn_mask, lf)\n","\n","            output = output.reshape(-1, output.shape[2])\n","            lf = lf.reshape(-1)\n","\n","            loss = criterion(output, lf)\n","            loss.backward()\n","            epoch_loss.append(loss.item())\n","        \n","            total_loss += loss.item()\n","            optimizer.step()\n","\n","        end1 = time()\n","        print(f\"Epoch {epoch}, Loss = {total_loss/len(train_loader)}\")\n","\n","        dev_loss =[]\n","        model.eval()\n","        for i, batch in enumerate(dev_loader):\n","            prb = batch[\"problem\"].to(device)\n","            lf = batch[\"linear_formula\"].to(device)\n","            attn_mask = batch[\"problem_attn_mask\"].to(device)\n","\n","            output, _ = model(prb, attn_mask, lf)\n","\n","            output = output.reshape(-1, output.shape[2])\n","            lf = lf.reshape(-1)\n","\n","            loss = criterion(output, lf)\n","            dev_loss.append(loss.item())\n","\n","        end2 = time()\n","        avg_dev_loss = np.mean(dev_loss)\n","        avg_epoch_loss = np.mean(epoch_loss)\n","\n","        loss_tracker[\"train\"].append(avg_epoch_loss)\n","        loss_tracker[\"dev\"].append(avg_dev_loss)\n","\n","        print(f\"Epoch {epoch}, Train Loss = {avg_epoch_loss}, Dev Loss = {avg_dev_loss}\")\n","        # print(f\"Epoch Time = {end2 - end1}\")\n","        time_tracker[\"train\"].append(round((end1 - start)/60 , 2))\n","        with open(os.path.join(args.result_dir, \"loss_tracker{}.json\".format(args.model_type)), \"w\") as outfile:\n","            json.dump(loss_tracker, outfile)\n","\n","        torch.save(model, os.path.join(args.checkpoint_dir, \"latest_checkpoint_{}.pth\".format(args.model_type)))\n","\n","        model_state = {\n","                'epoch': epoch,\n","                'train_loss' : avg_epoch_loss,\n","                'prev_best_epoch': best_epoch\n","        }\n","\n","        with open(os.path.join(args.checkpoint_dir, \"latest_chkpt_status_{}.json\".format(args.model_type)), \"w\") as outfile:\n","            json.dump(model_state, outfile)\n","\n","        #save the model whose loss is minimum\n","        if avg_dev_loss < min_loss:\n","            min_loss = avg_dev_loss\n","            best_epoch = epoch\n","            torch.save(model, os.path.join(args.checkpoint_dir, \"best_checkpoint_{}.pth\".format(args.model_type)))\n","            print(\"Best Model saved at epoch = \", epoch)\n","\n","    print(\"Training and Dev Complete for Bert2SeqAttn model\")\n","    #all epochs done\n","    #load the best model and return\n","    best_model = torch.load(os.path.join(args.checkpoint_dir, \"best_checkpoint_{}.pth\".format(args.model_type)))\n","    return best_model"]},{"cell_type":"markdown","metadata":{},"source":["TEST "]},{"cell_type":"code","execution_count":20,"metadata":{"execution":{"iopub.execute_input":"2024-04-03T19:44:01.237853Z","iopub.status.busy":"2024-04-03T19:44:01.237551Z","iopub.status.idle":"2024-04-03T19:44:01.250334Z","shell.execute_reply":"2024-04-03T19:44:01.249601Z","shell.execute_reply.started":"2024-04-03T19:44:01.237830Z"},"trusted":true},"outputs":[],"source":["def test_S2S(args,model):\n","    print(device)\n","#     args = ARGS()\n","    pth = \"/kaggle/input/math-data\"\n","#     args.processed_data = os.path.join(pth, \"processed_data\")\n","#     args.data_dir = os.path.join(pth, \"data\")\n","    print(args.processed_data)\n","\n","    test_dataset = TextToMathDataset(args.processed_data, \"test\")\n","    test_loader = DataLoader(test_dataset, batch_size = args.batch_size, shuffle = False, num_workers = args.num_workers, collate_fn=collate)\n","    dev_dataset = TextToMathDataset(args.processed_data, \"dev\")\n","    dev_loader = DataLoader(test_dataset, batch_size = args.batch_size, shuffle = False, num_workers = args.num_workers, collate_fn=collate)\n","    de_word2idx = test_dataset.de_word2idx\n","    en_word2idx = test_dataset.en_word2idx\n","    \n","    de_idx2word = test_dataset.de_idx2word\n","    en_idx2word = test_dataset.en_idx2word\n","    # decoder_hidden_units = model.decoder_hidden_units\n","    #------------------------------------------------\n","    # model = torch.load(os.path.join(args.checkpoint_dir, \"latest_checkpoint_Seq2Seq.pth\")).to(device) #load the latest checkpoint\n","    # model = torch.load(os.path.join(args.checkpoint_dir, \"best_checkpoint_Seq2Seq.pth\")).to(device) #load the best checkpoint\n","    # my_file = \".json\"\n","    model.eval()\n","    criterion = nn.CrossEntropyLoss(ignore_index = 0)\n","    loss_tracker = defaultdict(list)\n","\n","    start_token = de_word2idx[\"<sos>\"]\n","    end_token = de_word2idx[\"<eos>\"]\n","    print(f\" Start Token {start_token} and end token {end_token}\")\n","    for i, batch in enumerate(test_loader):\n","        problem = batch['problem'].to(device)\n","        linear_formula = batch['linear_formula'].to(device)\n","        answer = batch['answer'].to(device)\n","\n","        batch_size = problem.shape[0]\n","        max_target_len = linear_formula.shape[1]\n","        #max_target_len = 500\n","\n","        words = torch.zeros(batch_size, max_target_len).to(device)\n","        output ,(hidden, cell) = model.encoder(problem)\n","\n","        #beam search\n","        for b in range(batch_size):\n","            print(f\" i : {i} , b : {b}\")\n","            words[b,:] = beam_search(args, model , hidden[:,b,:].unsqueeze(1), cell[:,b,:].unsqueeze(1), start_token, end_token, max_target_len = max_target_len, beam_size = 10)\n","        convert_idx_sentence(args, words, problem, linear_formula, answer, de_idx2word, en_idx2word, \"test\")\n","    # print(\"Running evaluation script for test ... \")\n","    # subprocess.call(f\"python3 evaluator.py {my_file}\")\n","    print(\"Testing Complete. JSON created\")\n","    return\n"]},{"cell_type":"code","execution_count":21,"metadata":{"execution":{"iopub.execute_input":"2024-04-03T19:44:01.251605Z","iopub.status.busy":"2024-04-03T19:44:01.251355Z","iopub.status.idle":"2024-04-03T19:44:01.264563Z","shell.execute_reply":"2024-04-03T19:44:01.263865Z","shell.execute_reply.started":"2024-04-03T19:44:01.251584Z"},"trusted":true},"outputs":[],"source":["def test_S2S_Atten(args, model):\n","    print(device)\n","#     args = ARGS()\n","    pth = \"/kaggle/input/math-data\"\n","#     args.processed_data = os.path.join(pth, \"processed_data\")\n","#     args.data_dir = os.path.join(pth, \"data\")\n","    print(args.processed_data)\n","\n","    test_dataset = TextToMathDataset(args.processed_data, \"test\")\n","    test_loader = DataLoader(test_dataset, batch_size = args.batch_size, shuffle = False, num_workers = args.num_workers, collate_fn=collate)\n","    de_word2idx = test_dataset.de_word2idx\n","    en_word2idx = test_dataset.en_word2idx\n","    \n","    de_idx2word = test_dataset.de_idx2word\n","    en_idx2word = test_dataset.en_idx2word\n","    # decoder_hidden_units = model.decoder_hidden_units\n","    #------------------------------------------------\n","    # model = torch.load(os.path.join(args.checkpoint_dir, \"latest_checkpoint_Seq2Seq.pth\")).to(device) #load the latest checkpoint\n","    # model = torch.load(os.path.join(args.checkpoint_dir, \"best_checkpoint_Seq2Seq.pth\")).to(device) #load the best checkpoint\n","    # my_file = \".json\"\n","    model.eval()\n","    criterion = nn.CrossEntropyLoss(ignore_index = 0)\n","    loss_tracker = defaultdict(list)\n","\n","    start_token = de_word2idx[\"<sos>\"]\n","    end_token = de_word2idx[\"<eos>\"]\n","    print(\"start token : \" , start_token)\n","    for i, batch in enumerate(test_loader):\n","        problem = batch['problem'].to(device)\n","        linear_formula = batch['linear_formula'].to(device)\n","        answer = batch['answer'].to(device)\n","\n","        batch_size = problem.shape[0]\n","        max_target_len = linear_formula.shape[1]\n","        #max_target_len = 500\n","\n","        words = torch.zeros(batch_size, max_target_len).to(device)\n","        output, (hidden,cell)= model.encoder(problem)\n","    \n","        #beam search\n","        for b in range(batch_size):\n","#             print(f\" i : {i} , b : {b}\")\n","            words[b,:] = beam_search_attn_decoder(args, model , output[b,:,:].unsqueeze(0), hidden[:,b,:].unsqueeze(1), cell[:,b,:].unsqueeze(1), start_token, end_token, max_target_len = max_target_len, beam_size = 10)\n","        convert_idx_sentence(args, words, problem, linear_formula, answer, de_idx2word, en_idx2word, \"test\")\n","    # print(\"Running evaluation script for test ... \")\n","    # subprocess.call(f\"python3 evaluator.py {my_file}\")\n","    print(\"Testing Complete. JSON created\")\n","    return\n"]},{"cell_type":"code","execution_count":22,"metadata":{"execution":{"iopub.execute_input":"2024-04-03T19:44:01.266218Z","iopub.status.busy":"2024-04-03T19:44:01.265684Z","iopub.status.idle":"2024-04-03T19:44:01.277403Z","shell.execute_reply":"2024-04-03T19:44:01.276547Z","shell.execute_reply.started":"2024-04-03T19:44:01.266170Z"},"trusted":true},"outputs":[],"source":["def test_BertS2SAtten(model):\n","    print(device)\n","    args = ARGS()\n","    pth = \"/kaggle/input/math-data\"\n","    args.processed_data = os.path.join(pth, \"processed_data\")\n","    args.data_dir = os.path.join(pth, \"data\")\n","    print(args.processed_data)\n","\n","    test_dataset = Text2MathBertDataset(args.processed_data, \"test\")\n","    test_loader = DataLoader(test_dataset, batch_size = args.batch_size, shuffle = False, num_workers = args.num_workers, collate_fn=collate_bert)\n","    de_word2idx = test_dataset.de_word2idx\n","    en_word2idx = test_dataset.en_word2idx\n","    # decoder_hidden_units = model.decoder_hidden_units\n","    #------------------------------------------------\n","    # model = torch.load(os.path.join(args.checkpoint_dir, \"latest_checkpoint_Seq2Seq.pth\")).to(device) #load the latest checkpoint\n","    # model = torch.load(os.path.join(args.checkpoint_dir, \"best_checkpoint_Seq2Seq.pth\")).to(device) #load the best checkpoint\n","    # my_file = \".json\"\n","    model.eval()\n","    criterion = nn.CrossEntropyLoss(ignore_index = 0)\n","    loss_tracker = defaultdict(list)\n","\n","    start_token = de_word2idx[\"<sos>\"]\n","    end_token = de_word2idx[\"<eos>\"]\n","    for i, batch in enumerate(test_loader):\n","        problem = batch['problem'].to(device)\n","        linear_formula = batch['linear_formula'].to(device)\n","        answer = batch['answer'].to(device)\n","\n","\n","        attn_mask = batch['problem_attn_mask'].to(device)\n","        batch_size = problem.shape[0]\n","        max_target_len = linear_formula.shape[1]\n","        #max_target_len = 500\n","\n","        output = model.encoder(problem, attn_mask)\n","        hidden = torch.zeros(1, batch_size, model.decoder_hidden_units).to(device)\n","        cell = torch.zeros(1, batch_size, model.decoder_hidden_units).to(device)\n","\n","\n","        for b in range(batch_size):\n","            words = beam_search_attn_decoder(args, model , output[b,:,:].unsqueeze(0), hidden[:,b,:].unsqueeze(1), cell[:,b,:].unsqueeze(1), start_token, end_token, max_target_len = max_target_len, beam_size = 10)\n","        convert_idx_sentence(args, words, problem, linear_formula, answer, de_word2idx, en_word2idx, \"test\")"]},{"cell_type":"markdown","metadata":{},"source":["EVALUATOR RUN"]},{"cell_type":"code","execution_count":23,"metadata":{"execution":{"iopub.execute_input":"2024-04-03T19:44:01.278739Z","iopub.status.busy":"2024-04-03T19:44:01.278427Z","iopub.status.idle":"2024-04-03T19:44:01.289841Z","shell.execute_reply":"2024-04-03T19:44:01.288976Z","shell.execute_reply.started":"2024-04-03T19:44:01.278709Z"},"trusted":true},"outputs":[],"source":["# import gc \n","# gc.collect()"]},{"cell_type":"code","execution_count":24,"metadata":{"execution":{"iopub.execute_input":"2024-04-03T19:44:01.291750Z","iopub.status.busy":"2024-04-03T19:44:01.290960Z","iopub.status.idle":"2024-04-03T19:44:01.299304Z","shell.execute_reply":"2024-04-03T19:44:01.298462Z","shell.execute_reply.started":"2024-04-03T19:44:01.291726Z"},"trusted":true},"outputs":[],"source":["# torch.cuda.empty_cache()"]},{"cell_type":"markdown","metadata":{},"source":["MAIN"]},{"cell_type":"markdown","metadata":{},"source":[]},{"cell_type":"code","execution_count":27,"metadata":{"execution":{"iopub.execute_input":"2024-04-03T19:51:27.828368Z","iopub.status.busy":"2024-04-03T19:51:27.827512Z","iopub.status.idle":"2024-04-03T20:18:22.644248Z","shell.execute_reply":"2024-04-03T20:18:22.642771Z","shell.execute_reply.started":"2024-04-03T19:51:27.828332Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["cuda\n","/kaggle/input/math-data/processed_data\n","/kaggle/input/math-data/processed_data/train.json\n","Dataset Length = 19791\n","Encoder Vocab Size = 9998, Decoder Vocab Size = 113\n","Encoder word2idx Size = 9998, Decoder word2idx Size = 113\n","/kaggle/input/math-data/processed_data/dev.json\n","Dataset Length = 2961\n","Encoder Vocab Size = 9998, Decoder Vocab Size = 113\n","Encoder word2idx Size = 9998, Decoder word2idx Size = 113\n","Loading GloVe embeddings...\n"]},{"name":"stderr","output_type":"stream","text":["/tmp/ipykernel_34/982655231.py:24: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  embed_matrix[v] = torch.tensor(glove.vectors[glove.stoi[k]])\n"]},{"name":"stdout","output_type":"stream","text":["Loading Encoder...\n","Loading Seq2Seq LSTM Decoder...\n","\n","\n","-------------------Epoch =  0 ------------------------------\n","\n","Epoch 0, Loss = 0.5183365292762901\n","Epoch 0, Train Loss = 0.5183365292762901, Dev Loss = 0.4364081326351371\n","Best Model saved at epoch =  0\n","\n","\n","-------------------Epoch =  1 ------------------------------\n","\n","Epoch 1, Loss = 0.4363763352106769\n","Epoch 1, Train Loss = 0.4363763352106769, Dev Loss = 0.41652331989939495\n","Best Model saved at epoch =  1\n","\n","\n","-------------------Epoch =  2 ------------------------------\n","\n","Epoch 2, Loss = 0.4287314866586725\n","Epoch 2, Train Loss = 0.4287314866586725, Dev Loss = 0.4185991117390253\n","\n","\n","-------------------Epoch =  3 ------------------------------\n","\n","Epoch 3, Loss = 0.42042519652881993\n","Epoch 3, Train Loss = 0.42042519652881993, Dev Loss = 0.4109187445012472\n","Best Model saved at epoch =  3\n","\n","\n","-------------------Epoch =  4 ------------------------------\n","\n","Epoch 4, Loss = 0.41545360371687107\n","Epoch 4, Train Loss = 0.41545360371687107, Dev Loss = 0.4021707973493043\n","Best Model saved at epoch =  4\n","\n","\n","-------------------Epoch =  5 ------------------------------\n","\n","Epoch 5, Loss = 0.4126142817571976\n","Epoch 5, Train Loss = 0.4126142817571976, Dev Loss = 0.39369034478741305\n","Best Model saved at epoch =  5\n","\n","\n","-------------------Epoch =  6 ------------------------------\n","\n","Epoch 6, Loss = 0.4088804610855938\n","Epoch 6, Train Loss = 0.4088804610855938, Dev Loss = 0.3904200013606779\n","Best Model saved at epoch =  6\n","\n","\n","-------------------Epoch =  7 ------------------------------\n","\n","Epoch 7, Loss = 0.4049437681577125\n","Epoch 7, Train Loss = 0.4049437681577125, Dev Loss = 0.38687654929135434\n","Best Model saved at epoch =  7\n","\n","\n","-------------------Epoch =  8 ------------------------------\n","\n","Epoch 8, Loss = 0.39573419334815274\n","Epoch 8, Train Loss = 0.39573419334815274, Dev Loss = 0.38649956273135316\n","Best Model saved at epoch =  8\n","\n","\n","-------------------Epoch =  9 ------------------------------\n","\n","Epoch 9, Loss = 0.3915181237126976\n","Epoch 9, Train Loss = 0.3915181237126976, Dev Loss = 0.3758962513298117\n","Best Model saved at epoch =  9\n","\n","\n","-------------------Epoch =  10 ------------------------------\n","\n","Epoch 10, Loss = 0.38898573732770897\n","Epoch 10, Train Loss = 0.38898573732770897, Dev Loss = 0.3750160290028459\n","Best Model saved at epoch =  10\n","\n","\n","-------------------Epoch =  11 ------------------------------\n","\n","Epoch 11, Loss = 0.3852216526986324\n","Epoch 11, Train Loss = 0.3852216526986324, Dev Loss = 0.37180732879587397\n","Best Model saved at epoch =  11\n","\n","\n","-------------------Epoch =  12 ------------------------------\n","\n","Epoch 12, Loss = 0.37569125858474045\n","Epoch 12, Train Loss = 0.37569125858474045, Dev Loss = 0.3657026072984101\n","Best Model saved at epoch =  12\n","\n","\n","-------------------Epoch =  13 ------------------------------\n","\n","Epoch 13, Loss = 0.3731482641979065\n","Epoch 13, Train Loss = 0.3731482641979065, Dev Loss = 0.3598179652165341\n","Best Model saved at epoch =  13\n","\n","\n","-------------------Epoch =  14 ------------------------------\n","\n","Epoch 14, Loss = 0.36830219714292994\n","Epoch 14, Train Loss = 0.36830219714292994, Dev Loss = 0.3622821204123958\n","Training and Dev Complete for Seq2Seq model\n","Dev evaluater ...\n","Dev test\n","/kaggle/input/math-data/processed_data/dev.json\n","Dataset Length = 2961\n","Encoder Vocab Size = 9998, Decoder Vocab Size = 113\n","Encoder word2idx Size = 9998, Decoder word2idx Size = 113\n","Evaluating model on val data on given model\n"]},{"ename":"KeyboardInterrupt","evalue":"","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","Cell \u001b[0;32mIn[27], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m----> 2\u001b[0m     args, model \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_S2S\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      3\u001b[0m     test_S2S(args,model)\n","Cell \u001b[0;32mIn[17], line 113\u001b[0m, in \u001b[0;36mtrain_S2S\u001b[0;34m()\u001b[0m\n\u001b[1;32m    111\u001b[0m \u001b[38;5;66;03m#     evaluator_dev(args, best_model)\u001b[39;00m\n\u001b[1;32m    112\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDev test\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 113\u001b[0m     \u001b[43mevaluator_dev\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbest_model\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    114\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m args , best_model\n","Cell \u001b[0;32mIn[15], line 39\u001b[0m, in \u001b[0;36mevaluator_dev\u001b[0;34m(args, model)\u001b[0m\n\u001b[1;32m     36\u001b[0m         \u001b[38;5;66;03m#beam search\u001b[39;00m\n\u001b[1;32m     37\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m b \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(batch_size):\n\u001b[1;32m     38\u001b[0m \u001b[38;5;66;03m#             print(f\"at i: {i} and inside batch :{b}\")\u001b[39;00m\n\u001b[0;32m---> 39\u001b[0m             words[b,:] \u001b[38;5;241m=\u001b[39m \u001b[43mbeam_search\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m \u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhidden\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43mb\u001b[49m\u001b[43m,\u001b[49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43munsqueeze\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcell\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43mb\u001b[49m\u001b[43m,\u001b[49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43munsqueeze\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstart_token\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mend_token\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_target_len\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mmax_target_len\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbeam_size\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     40\u001b[0m         \u001b[38;5;66;03m# convert_idx_sentence(args, words, problem, linear_formula, answer)\u001b[39;00m\n\u001b[1;32m     41\u001b[0m         convert_idx_sentence(args, words, problem, linear_formula, answer, de_idx2word, en_idx2word, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdev\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n","Cell \u001b[0;32mIn[12], line 21\u001b[0m, in \u001b[0;36mbeam_search\u001b[0;34m(args, model, en_ht, en_ct, start_token, end_token, max_target_len, beam_size)\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m j \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(beam_size):\n\u001b[1;32m     20\u001b[0m     new_word_idx \u001b[38;5;241m=\u001b[39m top_inds[\u001b[38;5;241m0\u001b[39m][j]                \n\u001b[0;32m---> 21\u001b[0m     new_seq \u001b[38;5;241m=\u001b[39m sequence \u001b[38;5;241m+\u001b[39m [\u001b[43mnew_word_idx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m]\n\u001b[1;32m     22\u001b[0m     new_word_prob \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mlog(top_vals[\u001b[38;5;241m0\u001b[39m][j])\n\u001b[1;32m     23\u001b[0m     updated_score \u001b[38;5;241m=\u001b[39m score \u001b[38;5;241m-\u001b[39m new_word_prob\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}],"source":["if __name__ == \"__main__\":\n","    args, model = train_S2S()\n","    test_S2S(args,model)"]}],"metadata":{"kaggle":{"accelerator":"gpu","dataSources":[{"datasetId":4705850,"sourceId":7993233,"sourceType":"datasetVersion"}],"dockerImageVersionId":30674,"isGpuEnabled":true,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.12.2"}},"nbformat":4,"nbformat_minor":4}
